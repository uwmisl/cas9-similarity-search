{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder Training\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-25_blvqb because the default path (/tf/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "%pylab notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib as plt\n",
    "\n",
    "import primo.models\n",
    "import primo.datasets\n",
    "import primo.tools.sequences as seqtools\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from primo.models.cas9_keras import log10_crispr_spec, log10_norm_crispr_spec, linear_crispr_spec, dotproduct_crispr_spec, dotproduct_linearized\n",
    "\n",
    "from primo.models.encoder import entropy_regularizer\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "switching to train_9.h5 and train_1.h5\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OpenImagesVal' object has no attribute 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6d07ac2b7b60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mval_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_batch_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6d07ac2b7b60>\u001b[0m in \u001b[0;36mkeras_batch_generator\u001b[0;34m(dataset_batch_generator)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# indices: a positive integer uniquely identifying an image. This index is obtained by enumerating all the images in the dataset (before splitting them into test/train/validate datasets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# pairs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriplets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_batch_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mtriplets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriplets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/primo/primo/datasets/dataset.py\u001b[0m in \u001b[0;36mtriplet_batch_generator\u001b[0;34m(dataset_batch_generator, similarity_threshold)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# Get a batch worth of anchors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0manchor_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_batch_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/primo/primo/datasets/dataset.py\u001b[0m in \u001b[0;36mrandom_features\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrandom_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mfeature_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OpenImagesVal' object has no attribute 'path'"
     ]
    }
   ],
   "source": [
    "train_dataset = primo.datasets.OpenImagesTrain(\n",
    "    '/tf/open_images/train/', switch_every=10**5\n",
    ")\n",
    "validation_dataset = primo.datasets.OpenImagesVal('/tf/open_images/validation/')\n",
    "\n",
    "# To see how this value was derived, please consult the Materials and Methods subsection under Feature Extraction section.\n",
    "similarity_threshold = 75\n",
    "# Intuitively determined:\n",
    "batch_size = 150\n",
    "val_batch_size = 150\n",
    "\n",
    "\n",
    "def keras_batch_generator(dataset_batch_generator):\n",
    "    # Yield datasets\n",
    "    while True:\n",
    "        # This tuple contains:\n",
    "        # indices: a positive integer uniquely identifying an image. This index is obtained by enumerating all the images in the dataset (before splitting them into test/train/validate datasets)\n",
    "        # pairs:\n",
    "        indices, triplets = next(dataset_batch_generator)\n",
    "        yield triplets, np.zeros(len(triplets))\n",
    "\n",
    "train_batch_generator = keras_batch_generator(\n",
    "    primo.datasets.dataset.triplet_batch_generator(\n",
    "        train_dataset.random_features(batch_size),\n",
    "        similarity_threshold\n",
    "    )\n",
    ")\n",
    "\n",
    "val_batch_generator = keras_batch_generator(\n",
    "    primo.datasets.dataset.triplet_batch_generator(\n",
    "        validation_dataset.random_features(val_batch_size),\n",
    "        similarity_threshold\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "train_inputs, train_targets = next(train_batch_generator)\n",
    "val_inputs, val_targets = next(val_batch_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reserve space on the GPU for running simulations. It's important to do this before running any tensorflow code (which will take all available GPU memory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the training and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = primo.datasets.OpenImagesTrain(\n",
    "    '/tf/open_images/train/', switch_every=10**5\n",
    ")\n",
    "\n",
    "validation_dataset = primo.datasets.OpenImagesVal('/tf/open_images/validation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_batch_generator(dataset_batch_generator, similarity_threshold):\n",
    "    # Yield datasets\n",
    "    while True:\n",
    "        indices, pairs = next(dataset_batch_generator)\n",
    "        # The Euclidean distances between the two vectors in each pair\n",
    "        distances = np.sqrt(np.square(pairs[:,0,:] - pairs[:,1,:]).sum(1))\n",
    "        # Whether or not the images in this pair should be considered 'similar'. This is a boolean value, represented by an int (0 or 1), and is determined by whether the aforementioned Euclidean distances between image feature vectors are under some pre-deterined \"similarity threshold\".\n",
    "        similar = (distances < similarity_threshold).astype(int)\n",
    "        # Yield a pair of sequences, and 0-or-1 indicating whether they're similar.\n",
    "        yield pairs, similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see how this value was derived, please consult the \n",
    "# Materials and Methods subsection under Feature Extraction section in Bee et al. 2021\n",
    "similarity_threshold = 75\n",
    "\n",
    "# Intuitively determined:\n",
    "encoder_training_dataset_batch_size = 100\n",
    "# Intuitively determined:\n",
    "encoder_validation_dataset_batch_size = 2500\n",
    "\n",
    "encoder_train_batches = keras_batch_generator(\n",
    "    train_dataset.balanced_pairs(encoder_training_dataset_batch_size, similarity_threshold),\n",
    "    similarity_threshold\n",
    ")\n",
    "\n",
    "encoder_val_batches = keras_batch_generator(\n",
    "    validation_dataset.random_pairs(encoder_validation_dataset_batch_size),\n",
    "    similarity_threshold\n",
    ")\n",
    "\n",
    "predictor_train_batch_size = 1000\n",
    "predictor_train_batches = train_dataset.random_pairs(predictor_train_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the models and stack them together with the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = primo.models.Encoder('/tf/primo/data/models/encoder-function-P.h5')\n",
    "\n",
    "# TODO: Replace the yield_predictor with the nucleaseq Cas9 predictor, use that here instead. https://github.com/uwmisl/cas9-similarity-search/issues/3 \n",
    "#yield_predictor = primo.models.PredictorModel('/tf/primo/data/models/yield-model.h5')\n",
    "yield_predictor = primo.models.PredictorFunction()\n",
    "encoder.model.compile()\n",
    "yield_predictor.model.compile()\n",
    "encoder_trainer = primo.models.EncoderTrainer(encoder, yield_predictor)\n",
    "encoder_trainer.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_trainer.model.compile(tf.keras.optimizers.Adagrad(1e-3), 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = encoder_trainer.model.fit_generator(\n",
    "    encoder_train_batches,\n",
    "    steps_per_epoch = 1000,\n",
    "    epochs = 100,\n",
    "    validation_data = encoder_val_batches,\n",
    "    validation_steps = 1,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save('/tf/primo/data/models/encoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
