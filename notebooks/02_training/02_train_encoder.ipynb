{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder Training\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-ygg0zyjw because the default path (/tf/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import primo.models\n",
    "import primo.datasets\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from primo.models.encoder_trainer import cas9_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reserve space on the GPU for running simulations. It's important to do this before running any tensorflow code (which will take all available GPU memory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the training and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = primo.datasets.OpenImagesTrain(\n",
    "    '/tf/open_images/train/', switch_every=10**5\n",
    ")\n",
    "\n",
    "validation_dataset = primo.datasets.OpenImagesVal('/tf/open_images/validation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_batch_generator(dataset_batch_generator, similarity_threshold):\n",
    "    # Yield datasets\n",
    "    # TODO: Verify with Callie this understanding is correct https://github.com/uwmisl/cas9-similarity-search/issues/2\n",
    "    while True:\n",
    "        # This tuple contains:\n",
    "        # indices: a positive integer uniquely identifying an image. This index is obtained by enumerating all the images in the dataset (before splitting them into test/train/validate datasets)\n",
    "        # pairs:\n",
    "        indices, pairs = next(dataset_batch_generator)\n",
    "        # The Euclidean distances between the two vectors in each pair\n",
    "        distances = np.sqrt(np.square(pairs[:,0,:] - pairs[:,1,:]).sum(1))\n",
    "        # Whether or not the images in this pair should be considered 'similar'. This is a boolean value, represented by an int (0 or 1), and is determined by whether the aforementioned Euclidean distances between image feature vectors are under some pre-deterined \"similarity threshold\".\n",
    "        similar = (distances < similarity_threshold).astype(int)\n",
    "        # Yield a pair of sequences, and 0-or-1 indicating whether they're similar.\n",
    "        yield pairs, similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see how this value was derived, please consult the Materials and Methods subsection under Feature Extraction section.\n",
    "similarity_threshold = 75\n",
    "# Intuitively determined:\n",
    "encoder_training_dataset_batch_size = 100\n",
    "# Intuitively determined:\n",
    "encoder_validation_dataset_batch_size = 2500\n",
    "\n",
    "encoder_train_batches = keras_batch_generator(\n",
    "    train_dataset.balanced_pairs(encoder_training_dataset_batch_size, similarity_threshold),\n",
    "    similarity_threshold\n",
    ")\n",
    "\n",
    "# encoder_val_batches = keras_batch_generator(\n",
    "#     validation_dataset.random_pairs(encoder_validation_dataset_batch_size),\n",
    "#     similarity_threshold\n",
    "# )\n",
    "\n",
    "encoder_val_batches = keras_batch_generator(\n",
    "    validation_dataset.random_pairs(encoder_validation_dataset_batch_size),\n",
    "    similarity_threshold\n",
    ")\n",
    "\n",
    "# TODO: The new predictor is the nucleaseq Cas9 predictor. https://github.com/uwmisl/cas9-similarity-search/issues/3\n",
    "predictor_train_batch_size = 1000\n",
    "predictor_train_batches = train_dataset.random_pairs(predictor_train_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the models and stack them together with the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: Syntax error: word unexpected (expecting \")\")\r\n"
     ]
    }
   ],
   "source": [
    "# Yield predictor here is a differentiable DNA hybridization yield predictor (originally learned from the Nupack simulator). Represented in brown to the right of the one-hot box.\n",
    "![big](../../documentation/similarity_search_schematic.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2, 4096)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               ((None, 4096), (None 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Sequential)            (None, 20, 4)        12750928    lambda_1[0][0]                   \n",
      "                                                                 lambda_1[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 20, 4, 2)     0           encoder[0][0]                    \n",
      "                                                                 encoder[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 2, 20, 4)     0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None,)              0           lambda_4[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 12,750,928\n",
      "Trainable params: 12,750,928\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = primo.models.Encoder('/tf/primo/data/models/encoder-function-P.h5')\n",
    "#encoder = primo.models.Encoder()\n",
    "\n",
    "# TODO: Replace the yield_predictor with the nucleaseq Cas9 predictor, use that here instead. https://github.com/uwmisl/cas9-similarity-search/issues/3 \n",
    "#yield_predictor = primo.models.PredictorModel('/tf/primo/data/models/yield-model.h5')\n",
    "yield_predictor = primo.models.PredictorFunction()\n",
    "encoder.model.compile()\n",
    "yield_predictor.model.compile()\n",
    "encoder_trainer = primo.models.EncoderTrainer(encoder, yield_predictor)\n",
    "encoder_trainer.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$encoder_trainer.model.compile(tf.keras.optimizers.Adagrad(1e-2), tf.keras.metrics.BinaryCrossentropy(from_logits=False))\n",
    "encoder_trainer.model.compile(tf.keras.optimizers.Adagrad(1e-5), 'binary_crossentropy')\n",
    "#encoder_trainer.calcseq.compile(tf.keras.optimizers.Adagrad(1e-3), cas9_loss)\n",
    "#encoder_trainer.model.compile(tf.keras.optimizers.Adagrad(0), 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "switching to train_2.h5 and train_b.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "def compare_weights(w1, w2):\n",
    "    for i in range(len(w1)):\n",
    "        l1 = w1[i]\n",
    "        l2 = w2[i]\n",
    "        name = w1[i].name\n",
    "        dist = np.linalg.norm(l1 - l2)\n",
    "        max_change = np.max(l1 - l2)\n",
    "        print(f\"{name}: dist {dist}, max: {max_change}\")\n",
    "\n",
    "old_weights = None# deepcopy(encoder.model.trainable_weights)\n",
    "    \n",
    "import pandas as pd\n",
    "query_features = pd.read_hdf(\"/tf/primo/data/queries/features.h5\")\n",
    "test_feature = query_features.loc['luis_lego']\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global old_weights\n",
    "        # Save the model\n",
    "        filepath = f'/tf/primo/checkpoints/encoder-model.{epoch:02d}.h5'\n",
    "        encoder.save(filepath)\n",
    "\n",
    "        # Print outputs for checking\n",
    "        onehots = encoder.model.predict(np.array([test_feature]))\n",
    "        print(np.around(onehots, 4))\n",
    "        \n",
    "        # Print delta in weights to see if model is evolving\n",
    "        new_weights = deepcopy(encoder.model.trainable_weights)\n",
    "        if old_weights is not None:\n",
    "            compare_weights(old_weights, new_weights)\n",
    "        old_weights = new_weights\n",
    "\n",
    "callbacks = [\n",
    "    CustomCallback(),\n",
    "]\n",
    "# initial_results = encoder_trainer.model.evaluate(*next(encoder_train_batches))\n",
    "# initial_val_results = encoder_trainer.model.evaluate(*next(encoder_val_batches))\n",
    "# print(f\"Initial loss: {initial_results:.2f}\")\n",
    "# print(f\"Initial val loss: {initial_val_results:.2f}\")\n",
    "history = encoder_trainer.model.fit_generator(\n",
    "    encoder_train_batches,\n",
    "    steps_per_epoch = 1000,\n",
    "#    epochs = 100,\n",
    "#     steps_per_epoch = 500,\n",
    "    epochs = 200,\n",
    "    validation_data = encoder_val_batches,\n",
    "    validation_steps = 1,\n",
    "    verbose = 2,\n",
    "    callbacks = callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save('/tf/primo/data/models/encoder-function-P.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_trainer.model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique: 41 / 100\n",
    "Unique: 50 / 100import primo.tools.filepath as filepaths\n",
    "import primo.tools.sequences as seqtools\n",
    "import pandas as pd\n",
    "query_features_filepath = filepaths.get_query_features_path(isDocker=True)\n",
    "query_features = pd.read_hdf(query_features_filepath)\n",
    "query_seqs = encoder.encode_feature_seqs(query_features)\n",
    "print(f\"Query Seqs: {query_seqs}\")\n",
    "\n",
    "def seq_str_to_input(seq):\n",
    "    return np.transpose(seqtools.seqs_to_onehots(seq), [1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = encoder_trainer.model.predict(np.array([[query_features.loc['callie_janelle'], query_features.loc['callie_janelle']]]))\n",
    "print(f\"Full model: {a}\")\n",
    "\n",
    "b = encoder_trainer.predictor.model.predict(np.array([\n",
    "    np.concatenate([\n",
    "        seq_str_to_input('TAAAAAAAAAAAAGAAAAAA'),\n",
    "        seq_str_to_input('TAAAAAAAAAAAAGAAAAAA'),\n",
    "    ]),\n",
    "]))\n",
    "print(f\"Predictor with sequences: {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_trainer.calcdists.predict(np.array([[query_features.loc['callie_janelle'], query_features.loc['luis_lego']]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(encoder_train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = batch[0][12]\n",
    "seqs = np.array([encoder.model.predict(pair)])\n",
    "print(seqs.shape)\n",
    "print(encoder_trainer.predictor.model.predict(seqs))\n",
    "print(encoder_trainer.calcdists.predict(np.array([pair])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_trainer.predictor.model(np.array([\n",
    "    np.concatenate([\n",
    "        seq_str_to_input('TAAAAAAAAAAAAGAAAAAA'),\n",
    "        seq_str_to_input('TAAAAAAAAAAAAGAAAAAA'),\n",
    "    ]),\n",
    "    np.concatenate([\n",
    "        seq_str_to_input('GACATCAACGAACAAAGTAA'),\n",
    "        seq_str_to_input('GAAAACAAAAAAAAAAAAAA'),\n",
    "    ]),\n",
    "]))\n",
    "#print(np.transpose(seqtools.seqs_to_onehots('GAAAACAAAAAAAAAAAAAA'), [1, 0, 2]).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_trainer.model.predict(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
